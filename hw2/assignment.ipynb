{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval I #\n",
    "## Assignment 2: retrieval models [100 points + 10 bonus points] ##\n",
    "**TA**: Christophe Van Gysel (cvangysel@uva.nl; C3.258B, Science Park 904)\n",
    "\n",
    "**Secondary TAs**: Harrie Oosterhuis, Nikos Voskarides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic information retrieval concepts. You will implement and evaluate different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a VirtualBox image that comes pre-loaded with an index and a Python installation. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-two assignment**, the deadline is **23:59 - 25 January, 2017**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](https://piazza.com/class/ixoz63p156g1ts).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "This assignment comes pre-loaded on a VirtualBox running Ubuntu. We have configured the indexing software and Python environment such that it works out of the box. You are allowed to extract the files from the VirtualBox and set-up your own non-virtualized environment. However, in this case you are on your own w.r.t. software support.\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "      \n",
    "`Python + Jupyter`, `Indri`, `Gensim` and `Pyndri` come pre-installed (see `$HOME/.local`). TREC Eval can be found in `$HOME/Downloads/trec_eval.9.0`. The password of the `student` account on the VirtualBox is `datascience`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [45 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html). **[5 points]**\n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ð›Œ in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ð› [500, 1000, ..., 2000]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ð›… in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of â€œsoftâ€ passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ð›” equal to 50, and Dirichlet smoothing with ð› optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[10 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences.\n",
    "\n",
    "**NOTE**: Donâ€™t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # Needs to be installed\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import seaborn as sns # Needs to be installed\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import pickle\n",
    "from functools import partial\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "import pprofile\n",
    "import subprocess\n",
    "import gensim\n",
    "import cython\n",
    "import logging\n",
    "import pyndri.compat\n",
    "import os\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "########## COLLECTING DOCUMENT STATISTICS ################\n",
    "# Global variables, dictionaries from the index\n",
    "token2id, id2token, id2df = index.get_dictionary()\n",
    "id2colfreq = index.get_term_frequencies()\n",
    "id2colprob = {}\n",
    "start_index, end_index = index.document_base(), index.maximum_document()\n",
    "total_docs = end_index - start_index\n",
    "\n",
    "# Collection length and longest document\n",
    "longest_doc_length = -1\n",
    "collection_length = 0\n",
    "for i in range(start_index, end_index):\n",
    "    doc = index.document(i)\n",
    "    collection_length += len(doc[1])\n",
    "    longest_doc_length = max(longest_doc_length, len(doc[1]))\n",
    "\n",
    "# Average document length\n",
    "avg_doc_length = collection_length / total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_queries(filename='./ap_88_89/topics_title', raw_terms=False):\n",
    "    \"\"\"\n",
    "    Preprocess queries for O(1) access. \n",
    "    From lists of words, to dictionaries \n",
    "    of token_ids and their counts, and optionally\n",
    "    the raw terms used for word2vec/doc2vec, etc.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "    \n",
    "    processed_queries = OrderedDict()\n",
    "    global id2colprob\n",
    "    \n",
    "    # Preprocessing of all queries\n",
    "    for query_id in queries:\n",
    "        query = queries[query_id]\n",
    "\n",
    "        # Tokenize the query\n",
    "        tokenized_query = index.tokenize(query)\n",
    "\n",
    "        # Get token ids for tokens in the query\n",
    "        token_ids = [token2id.get(token, 0) for token in tokenized_query]\n",
    "\n",
    "        # Remove stopwords\n",
    "        token_ids = [t for t in token_ids if t > 0]\n",
    "\n",
    "        # Construct a dictionary of token_id -> count(token_id) to represent query\n",
    "        query_terms = Counter(token_ids)\n",
    "        \n",
    "        for query_t in query_terms:\n",
    "            if query_t not in id2colprob:\n",
    "                id2colprob[query_t] = id2colfreq[query_t] / collection_length\n",
    "        \n",
    "        query_info = {'terms': query_terms, 'length':len(query_terms)}\n",
    "        \n",
    "        if raw_terms:\n",
    "            # For word2vec/doc2vec, etc.\n",
    "            query_info['raw_terms'] = tuple([id2token[t_id] for t_id in query_terms if id2colfreq[t_id] > 5])\n",
    "        \n",
    "        processed_queries[query_id] = query_info\n",
    "    \n",
    "    return processed_queries\n",
    "\n",
    "# Get all queries to try models on \n",
    "queries = preprocess_queries()\n",
    "\n",
    "def process_docinfo(document):\n",
    "    \"\"\"\n",
    "    Given a document, returns information used by various\n",
    "    scoring measures.\n",
    "    \"\"\"\n",
    "    # Hashmap of words and counts for O(1) access\n",
    "    document_words = Counter(document[1])\n",
    "\n",
    "    # Frequency of most common word in the document.\n",
    "    most_common = document_words.most_common(1)[0][1]\n",
    "\n",
    "    # Length of the document\n",
    "    doc_length = len(document[1])\n",
    "\n",
    "    # Normalized document length\n",
    "    doc_norm_length = doc_length / avg_doc_length\n",
    "\n",
    "    # Number of unique tokens in the document (i.e. document vocabulary)\n",
    "    doc_unique = len(document_words)\n",
    "        \n",
    "    return {'terms': document_words, 'top_freq':most_common, 'text': document[1],\n",
    "            'length':doc_length, 'norm_length':doc_norm_length, 'unique':doc_unique, 'outside_id':document[0]}\n",
    "\n",
    "class TrecEvaluator:\n",
    "    \n",
    "    def __init__(self, validation_data, runfile_name=\"run\"):\n",
    "        self._runfile_name = runfile_name\n",
    "        self._validation_data = validation_data\n",
    "        \n",
    "    def evaluate(self, data, complete=False):\n",
    "        with open(self._runfile_name, \"w\") as f:\n",
    "            write_run(\n",
    "                model_name=self._runfile_name,\n",
    "                data=data,\n",
    "                out_f=f,\n",
    "                max_objects_per_query=1000)\n",
    "            \n",
    "        return self._call_trec_eval_complete() if complete else self._call_trec_eval() \n",
    "    \n",
    "    def _call_trec_eval(self):\n",
    "        \"\"\"\n",
    "        Runs the a shell script which should be present\n",
    "        in the same folder as the notebook. The script calls trec_eval,\n",
    "        with the model scores and returns the results.\n",
    "        \n",
    "        Returns only overall results.\n",
    "        \"\"\"\n",
    "        cmd = ['./run_trec_eval.sh', self._validation_data, self._runfile_name]\n",
    "        result = {}\n",
    "        try:\n",
    "            p = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
    "            for line in p.stdout:\n",
    "                frags = line.decode().strip().split()\n",
    "                result[frags[0]] = float(frags[2])\n",
    "            p.wait()\n",
    "        except Exception as e:\n",
    "            print('Error executing evaluation script. ', e)\n",
    "            print('If you are getting permission error from subprocess, use \"chmod +x run_trec_eval.sh\"')\n",
    "        return result\n",
    "    \n",
    "    def _call_trec_eval_complete(self):\n",
    "        \"\"\"\n",
    "        Returns the results for each query\n",
    "        as well as general results.\n",
    "        \n",
    "        Note: Please use chmod +x run_complete_trec_eval.sh if \n",
    "        you are running into Permission errors.\n",
    "        \"\"\"\n",
    "        cmd = ['./run_complete_trec_eval.sh', self._validation_data, self._runfile_name]\n",
    "        result = {}\n",
    "        p = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
    "        for line in p.stdout:\n",
    "            frags = line.decode().strip().split()\n",
    "            if frags[1] in result:\n",
    "                result[frags[1]].append({frags[0]: float(frags[2])})\n",
    "            else:\n",
    "                result[frags[1]] = [{frags[0]:float(frags[2])}]\n",
    "            p.wait()\n",
    "        return result\n",
    "\n",
    "# Evaluators for test and validation set.\n",
    "trec_eval = TrecEvaluator(validation_data=\"ap_88_89/qrel_validation\")\n",
    "trec_eval_test = TrecEvaluator(validation_data=\"ap_88_89/qrel_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augmented_termfreq(document, token_id, max_frequency):\n",
    "    \"\"\"\n",
    "    Returns augmented term frequency of a word in the document.\n",
    "    To prevent bias towards larger documents, we divide the \n",
    "    raw term frequency by the maximum raw term frequency of any\n",
    "    term in the document.\n",
    "    \n",
    "    Reference:\n",
    "    http://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html\n",
    "    \n",
    "    Args:\n",
    "    - document: A collections.Counter dictionary that maps token_ids to their counts.\n",
    "    - token_id: The unque id of the term, in the collection.\n",
    "    - max_frequency: The maximum raw frequency found in the document.\n",
    "    \"\"\"\n",
    "    term_count = document.get(token_id, 0)\n",
    "    \n",
    "    # Handle 0 case\n",
    "    if term_count == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Return augmented term frequency with smoothing of a = 0.4\n",
    "    # Alternative possible value is 0.5.\n",
    "    return 0.4 + 0.6 * (term_count / max_frequency)\n",
    "\n",
    "def inverse_docfreq(token_id):\n",
    "    \"\"\"\n",
    "    Returns the inverse document frequency of the term.\n",
    "    \n",
    "    Args:\n",
    "    - token_id: The unique id of the term, in the collection.\n",
    "    \"\"\"\n",
    "    df = id2df.get(token_id, 0)\n",
    "    return np.log(total_docs / df)\n",
    "\n",
    "def tf_idf(doc, query, token_id):\n",
    "    \"\"\"\n",
    "    Returns the term-frequency inverse document frequency of a term,\n",
    "    in a document. Calculated as tf(t,d) * idf(t,D).\n",
    "    \n",
    "    Args:\n",
    "    - doc: Dictionary containing document information.\n",
    "    - query: Dictionary containing query information.\n",
    "    - token_id: The unque id of the term, in the collection.\n",
    "    \"\"\"\n",
    "    # Compute augmented term frequency\n",
    "    term_freq = augmented_termfreq(doc['terms'], token_id, doc['top_freq'])\n",
    "    \n",
    "    if term_freq == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute inverse document frequency\n",
    "    idf = inverse_docfreq(token_id)\n",
    "    \n",
    "    return term_freq * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 ($k_1 = 1.2$, $b = 0.75$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bm25(doc, query, token_id):\n",
    "    \"\"\"\n",
    "    Computes the overal bm25 score which involves\n",
    "    computing the okapi frequency and multiplying it with the\n",
    "    IDF. This method serves as an 'interface' shared by the task 1\n",
    "    scoring methods.\n",
    "    \n",
    "    Args:\n",
    "    - doc: Dictionary containing document information.\n",
    "    - query: Dictionary containing query information.\n",
    "    - token_id: The unque id of the term, in the collection.\n",
    "    \"\"\"\n",
    "    return bm25_score(doc['terms'], token_id, doc['norm_length'])\n",
    "\n",
    "def okapi_frequency(doc_terms, token_id, norm_length, k1, b):\n",
    "    \"\"\"\n",
    "    Returns the okapi frequency of a term, given the hyperparameters\n",
    "    k1 and b.\n",
    "    \n",
    "    Args:\n",
    "    - doc_terms: Dictionary with the terms in the document, and their counts as values.\n",
    "    - token_id: The unque id of the term, in the collection.\n",
    "    - norm_length: Normalized length of the document (i.e. already divided by avg doc length.)\n",
    "    - k1,b: Hyperparameters tuning the importance of term count, doc length.\n",
    "    \"\"\"\n",
    "    term_count = doc_terms.get(token_id, 0)\n",
    "    return ((k1 + 1) * term_count) / (k1 * ((1 - b) + b * norm_length) + term_count)\n",
    "\n",
    "def bm25_score(doc_terms, token_id, norm_length, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    Computes overal bm 25 score.\n",
    "    \n",
    "    Args:\n",
    "    - doc_terms: Dictionary with the terms in the document, and their counts as values.\n",
    "    - token_id: The unque id of the term, in the collection.\n",
    "    - norm_length: Normalized length of the document (i.e. already divided by avg doc length.)\n",
    "    - k1,b: Hyperparameters tuning the importance of term count, doc length.\n",
    "    \"\"\"\n",
    "    return okapi_frequency(doc_terms, token_id, norm_length, k1, b) * inverse_docfreq(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language models\n",
    "\n",
    "General formula for computing scores given a document language model and query is:\n",
    "\n",
    "$$log P(q|d) = \\sum_{tf(q_i, d) > 0} log \\dfrac{P_s (q_i | d)}{\\alpha_d \\cdot P(q_i | C)} + n \\cdot log \\alpha_d$$\n",
    "\n",
    "where: \n",
    "\n",
    "- $\\alpha_d$ is a document dependent constant related to how much probability to allocate to unseen words, according to *a particular smoothing method.*\n",
    "- $P_s(q_i | d)$ is the probability of seeing the respective word given the smoothed document language model.\n",
    "- $n$ is the length of the query.\n",
    "- $P(q_i | C)$ is the probability of the word given the collection language model, which will simply be $\\dfrac{tf(q_i, C)}{\\|C\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jelinek_score(doc, query, token_id, lmbda=0.5):\n",
    "    \"\"\"\n",
    "    'Interface' method with common parameters shared by scoring methods.\n",
    "    Call this method to compute the jelinek smoothed score of a language model\n",
    "    given a document, query and term (token) id.\n",
    "    \"\"\"\n",
    "    term_doc_prob = doc['terms'].get(token_id, 0) / doc['length']\n",
    "    term_col_prob = id2colfreq[token_id] / collection_length\n",
    "    return jelinek_mercer(term_doc_prob, term_col_prob, lmbda)\n",
    "\n",
    "def jelinek_mercer(term_doc_prob, term_col_prob, doc_length, lmbda=0.5):\n",
    "    \"\"\"\n",
    "    Computes and returns the score of a query-document-term pair given a document language model\n",
    "    defined using the Jelinek-Mercer smoothing method.\n",
    "    \n",
    "    Ps = (1 - lambda) * document_prob + lambda * collection_prob\n",
    "    alpha_d = lambda\n",
    "    \n",
    "    Args:\n",
    "    - term_doc_prob: tf(q_i, d) / |d|. The unsmoothed estimate for the term in the document.\n",
    "    - term_col_prob: tf(q_i, C) / |C|. The term collection probability.\n",
    "    - doc_length: |d| - Total length of the document we are computing the score for.\n",
    "    - lmbda: Jelinek-Mercer interpolation parameter.\n",
    "    \"\"\"\n",
    "    # Collection smoothing.\n",
    "    collection_model = lmbda * term_col_prob\n",
    "    \n",
    "    # Smoothed seen probabililty.\n",
    "    seen_prob = ((1.-lmbda) * term_doc_prob) + collection_model\n",
    "    \n",
    "    # Final document score.\n",
    "    return np.log(seen_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dirichlet_score(doc, query, token_id, mu=500):\n",
    "    \"\"\"\n",
    "    Returns score for a languge model smoothed using dirichlet prior.\n",
    "    \"\"\"\n",
    "    term_doc_prob = doc['terms'].get(token_id, 0) / doc['length']\n",
    "    term_col_prob = id2colfreq[token_id] / collection_length\n",
    "    return dirichlet_smoothing(doc['terms'].get(token_id, 0), term_col_prob, doc['length'], query['length'], mu)\n",
    "\n",
    "def dirichlet_smoothing(term_doc_freq, term_col_prob, doc_length, query_length, mu=500):\n",
    "    \"\"\"\n",
    "    Score of a query-document pair given a document language model defined\n",
    "    using a Dirichlet Prior for smoothing.\n",
    "    \n",
    "    Args:\n",
    "    - term_doc_prob: tf(q_i, d) / |d|. The unsmoothed estimate for the term in the document.\n",
    "    - term_col_prob: tf(q_i, C) / |C|. The term collection probability.\n",
    "    - doc_length: |d| - Total length of the document we are computing the score for.\n",
    "    - query_length: Length of the query.\n",
    "    - mu: Dirichlet parameter.\n",
    "    \"\"\"\n",
    "    alpha_d = mu / (doc_length + mu)\n",
    "    \n",
    "    # Collection smoothing.\n",
    "    collection_model = alpha_d * term_col_prob\n",
    "    \n",
    "    # Smoothed seen probability\n",
    "    seen_prob = (term_doc_freq  + (mu * term_col_prob)) / (doc_length + mu)\n",
    "    \n",
    "    return np.log(seen_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abs_score(doc, query, token_id, delta=0.5):\n",
    "    \"\"\"\n",
    "    Return Absolute discount score.\n",
    "    \"\"\"\n",
    "    term_col_prob = id2colfreq[token_id] / collection_length\n",
    "    \n",
    "    return abs_disc(doc['terms'].get(token_id, 0), term_col_prob, doc['length'], doc['unique'], query['length'], delta)\n",
    "\n",
    "def abs_disc(term_doc_freq, term_col_prob, doc_length, doc_unique, query_length, delta=0.5):\n",
    "    \"\"\"\n",
    "    Score of a query-document pair given a document language model defined\n",
    "    using Absolute Discounting for smoothing.\n",
    "    \n",
    "    Args:\n",
    "    - term_doc_freq: tf(q_i, d). The number of occurances of the term in the document.\n",
    "    - term_col_prob: tf(q_i, C) / |C|. The term collection probability.\n",
    "    - doc_length: |d| - Total length of the document we are computing the score for.\n",
    "    - doc_unique: |d|_u - Number of unique tokens in the document.\n",
    "    - query_length: Length of the query.\n",
    "    - delta: Absolute discounting parameter.\n",
    "    \"\"\"\n",
    "    alpha_d = (delta * doc_unique) / doc_length\n",
    "    \n",
    "    # Collection\n",
    "    collection_model = alpha_d * term_col_prob\n",
    "    \n",
    "    # Smoothed doc prob\n",
    "    seen_prob = (max(term_doc_freq - delta, 0) / doc_length) + collection_model\n",
    "    \n",
    "    return np.log(seen_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing scores for each measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_score(doc, query, measure_func):\n",
    "    score = 0\n",
    "    for token_id in query['terms']:\n",
    "        score += measure_func(doc=doc, query=query, token_id=token_id)\n",
    "    return score\n",
    "\n",
    "def compute_measure(queries, measure_functions, verbose=True):\n",
    "    \"\"\"\n",
    "    Computes the score for every query-document pair\n",
    "    that have any word in common, using the 'measure_func'\n",
    "    function.\n",
    "    \n",
    "    Returns a dictionary where for each query_id the \n",
    "    values is an array with the top 1000 documents ranked\n",
    "    by their score according to 'measure_func'.\n",
    "    \n",
    "    Args:\n",
    "    - queries: Dictionary with key 'query_id' and values as dicts\n",
    "    of the terms in the query and their counts.\n",
    "    - documents: OrderedDict with keys 'doc_id' and processed docs as values.\n",
    "    - measure_functions: Measure functions that recieve a query and a document\n",
    "    and returns the score according to some predefined metric (e.g. tf-idf, etc.)\n",
    "    \"\"\"\n",
    "    # Initialize scores for all measurement functions\n",
    "    funcscores = []\n",
    "    for x in range(len(measure_functions)):\n",
    "        funcscores.append(defaultdict(list))\n",
    "    \n",
    "    for doc_id in range(start_index, end_index):\n",
    "        if verbose and doc_id % 10000 == 0:\n",
    "            print(\"Processed {0} documents.\".format(doc_id))\n",
    "            \n",
    "        doc = index.document(doc_id)\n",
    "        \n",
    "        if len(doc[1]) == 0:\n",
    "            continue\n",
    "            \n",
    "        document_info = process_docinfo(doc)\n",
    "\n",
    "        # For every query we have, compute the document score.\n",
    "        for query_id in queries:\n",
    "            query_info = queries[query_id]\n",
    "            \n",
    "            ### SIMPLIFYING ASSUMPTION ###\n",
    "            # If no words in common between query-doc, skip.\n",
    "            # This will leave only 6 queries, with < 1000 docs\n",
    "            # However all ranked documents will actually be relevant\n",
    "            # to the query. Please see paper for further details.\n",
    "            if len(set(query_info['terms']).intersection(document_info['terms'])) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute and store scores for each measure.\n",
    "            for func_index, measure_func in enumerate(measure_functions):\n",
    "                # Score for current doc-query pair.\n",
    "                pair_score = compute_score(document_info, query_info, measure_func)\n",
    "                curr_score = funcscores[func_index]\n",
    "                curr_score[query_id].append((pair_score, document_info['outside_id']))\n",
    "    \n",
    "    for score in funcscores:\n",
    "        for query_id in score:\n",
    "            score[query_id] = heapq.nlargest(1000, score[query_id], itemgetter(0))\n",
    "    \n",
    "    return funcscores\n",
    "\n",
    "# Save scores for later analysis, so we don't recompute/ hold in memory.\n",
    "tf_file = 'tfidf_scores.p'\n",
    "bm25_file = 'bm25_scores.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute scores for models that do not need hyperparameter optimization\n",
    "# i.e. tfidf and bm25.\n",
    "simple_scores = compute_measure(queries, measure_functions=[tf_idf, bm25])\n",
    "tfidfscore, bm25score = simple_scores[0], simple_scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF Validation set score is {'ndcg_cut_10': 0.349, 'recall_1000': 0.5749, 'P_5': 0.3467, 'map_cut_1000': 0.209}\n",
      "BM25 Validation set score is {'ndcg_cut_10': 0.3896, 'recall_1000': 0.6626, 'P_5': 0.3933, 'map_cut_1000': 0.24}\n",
      "TF_IDF Test set score is {'ndcg_cut_10': 0.3428, 'recall_1000': 0.6089, 'P_5': 0.3567, 'map_cut_1000': 0.1829}\n",
      "BM25 Test set score is {'ndcg_cut_10': 0.4109, 'recall_1000': 0.6493, 'P_5': 0.4217, 'map_cut_1000': 0.2183}\n"
     ]
    }
   ],
   "source": [
    "print('TF_IDF Validation set score is {0}'.format(trec_eval.evaluate(data=tfidfscore)))\n",
    "print('BM25 Validation set score is {0}'.format(trec_eval.evaluate(data=bm25score)))\n",
    "print('TF_IDF Test set score is {0}'.format(trec_eval_test.evaluate(data=tfidfscore)))\n",
    "print('BM25 Test set score is {0}'.format(trec_eval_test.evaluate(data=bm25score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(tf_file, 'wb') as fp:\n",
    "    pickle.dump(tfidfscore, fp)\n",
    "    \n",
    "with open(bm25_file, 'wb') as fp:\n",
    "    pickle.dump(bm25score, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "%reset_selective -f simple_scores\n",
    "%reset_selective -f tfidfscore\n",
    "%reset_selective -f bm25score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing hyperparameters for smoothed language models\n",
    "\n",
    "- We can optimize these parameters using either grid-search or a two-stage smoothing model. The former is implemented below for a range of values (it's essentially grid search with result evaluation on the validation set). The latter was not implemented due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 documents.\n",
      "Processed 20000 documents.\n",
      "Processed 30000 documents.\n",
      "Processed 40000 documents.\n",
      "Processed 50000 documents.\n",
      "Processed 60000 documents.\n",
      "Processed 70000 documents.\n",
      "Processed 80000 documents.\n",
      "Processed 90000 documents.\n",
      "Processed 100000 documents.\n",
      "Processed 110000 documents.\n",
      "Processed 120000 documents.\n",
      "Processed 130000 documents.\n",
      "Processed 140000 documents.\n",
      "Processed 150000 documents.\n",
      "Processed 160000 documents.\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Jelinek Mercer\n",
    "lambdas = np.arange(0.1,1.0,0.1)\n",
    "jelinek_funcs = [partial(jelinek_score, lmbda=lmbda_value) for lmbda_value in lambdas]\n",
    "jelinek_scores = compute_measure(queries, measure_functions=jelinek_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jelinek score for lambda 0.1 is {'P_5': 0.36, 'recall_1000': 0.6091, 'ndcg_cut_10': 0.3658, 'map_cut_1000': 0.2157}\n",
      "Jelinek score for lambda 0.2 is {'P_5': 0.3733, 'recall_1000': 0.6137, 'ndcg_cut_10': 0.3736, 'map_cut_1000': 0.2181}\n",
      "Jelinek score for lambda 0.30000000000000004 is {'P_5': 0.3733, 'recall_1000': 0.6257, 'ndcg_cut_10': 0.3683, 'map_cut_1000': 0.2187}\n",
      "Jelinek score for lambda 0.4 is {'P_5': 0.3933, 'recall_1000': 0.6298, 'ndcg_cut_10': 0.3759, 'map_cut_1000': 0.2197}\n",
      "Jelinek score for lambda 0.5 is {'P_5': 0.38, 'recall_1000': 0.6372, 'ndcg_cut_10': 0.3698, 'map_cut_1000': 0.2194}\n",
      "Jelinek score for lambda 0.6 is {'P_5': 0.3867, 'recall_1000': 0.6353, 'ndcg_cut_10': 0.3715, 'map_cut_1000': 0.2199}\n",
      "Jelinek score for lambda 0.7000000000000001 is {'P_5': 0.3733, 'recall_1000': 0.6337, 'ndcg_cut_10': 0.3788, 'map_cut_1000': 0.2201}\n",
      "Jelinek score for lambda 0.8 is {'P_5': 0.3867, 'recall_1000': 0.634, 'ndcg_cut_10': 0.3823, 'map_cut_1000': 0.2204}\n",
      "Jelinek score for lambda 0.9 is {'P_5': 0.3933, 'recall_1000': 0.624, 'ndcg_cut_10': 0.3868, 'map_cut_1000': 0.2175}\n"
     ]
    }
   ],
   "source": [
    "jelinek_trec_scores = []\n",
    "for i, score in enumerate(jelinek_scores):\n",
    "    trec_score = trec_eval.evaluate(data=score)\n",
    "    # Show score\n",
    "    print('Jelinek score for lambda {0} is {1}'.format(lambdas[i], trec_score))\n",
    "    trec_score['lambda'] = lambdas[i]\n",
    "    jelinek_trec_scores.append(trec_score)\n",
    "    \n",
    "# Save to not compute again\n",
    "jelinek_file = 'jelinek_validation.p'\n",
    "with open(jelinek_file, 'wb') as fp:\n",
    "    pickle.dump(jelinek_trec_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Jelinek-Mercer score using NDCG is 0.3868 with lambda of 0.9\n"
     ]
    }
   ],
   "source": [
    "# Load and plot jelinek scores for all parameter values\n",
    "# And select best model\n",
    "\n",
    "# This is last minute, so a bit contrived.\n",
    "pos = {lmbda:position for position, lmbda in enumerate(lambdas)}\n",
    "\n",
    "with open('jelinek_validation.p', 'rb') as fp:\n",
    "    jelinek_all_scores = pickle.load(fp)\n",
    "    \n",
    "jelinek_ndcg_10 = [0] * len(jelinek_all_scores)\n",
    "\n",
    "for score in jelinek_all_scores:\n",
    "    pos = lambdas.index(score['lambda'])\n",
    "    jelinek_ndcg_10[pos] = score['ndcg_cut_10']\n",
    "    \n",
    "best_ndcg = max(jelinek_ndcg_10)\n",
    "best_lambda = lambdas[jelinek_ndcg_10.index(max(jelinek_ndcg_10))]\n",
    "print('Best Jelinek-Mercer score using NDCG is {0} with lambda of {1}'.format(best_ndcg, best_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results of jelinek optimized on validation set are: \n",
      "\n",
      "{'recall_1000': 0.6288, 'P_5': 0.375, 'ndcg_cut_10': 0.3849, 'map_cut_1000': 0.1977}\n"
     ]
    }
   ],
   "source": [
    "# # Recompute Jelinek-Mercer module with best lambda, and present results on test set.\n",
    "best_jelinek_score = compute_measure(queries, measure_functions=[partial(jelinek_score, lmbda=best_lambda)], verbose=False)\n",
    "with open('best_jelinek_scores.p', 'wb') as fp:\n",
    "    pickle.dump(best_jelinek_score[0], fp)\n",
    "\n",
    "print('Test set results of jelinek optimized on validation set are: \\n')\n",
    "print(trec_eval_test.evaluate(data=best_jelinek_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Clean memory\n",
    "%reset # Note: you need to rerun some initial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 documents.\n",
      "Processed 20000 documents.\n",
      "Processed 30000 documents.\n",
      "Processed 40000 documents.\n",
      "Processed 50000 documents.\n",
      "Processed 60000 documents.\n",
      "Processed 70000 documents.\n",
      "Processed 80000 documents.\n",
      "Processed 90000 documents.\n",
      "Processed 100000 documents.\n",
      "Processed 110000 documents.\n",
      "Processed 120000 documents.\n",
      "Processed 130000 documents.\n",
      "Processed 140000 documents.\n",
      "Processed 150000 documents.\n",
      "Processed 160000 documents.\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Dirichlet Prior\n",
    "mus = np.arange(500,2500,500)\n",
    "dirichlet_funcs = [partial(dirichlet_score, mu=mu_value) for mu_value in mus]\n",
    "dirichlet_scores = compute_measure(queries, measure_functions=dirichlet_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirichlet score for mu 500 is {'P_5': 0.4067, 'recall_1000': 0.6203, 'ndcg_cut_10': 0.3936, 'map_cut_1000': 0.232}\n",
      "Dirichlet score for mu 1000 is {'P_5': 0.4, 'recall_1000': 0.6305, 'ndcg_cut_10': 0.404, 'map_cut_1000': 0.2397}\n",
      "Dirichlet score for mu 1500 is {'P_5': 0.4, 'recall_1000': 0.636, 'ndcg_cut_10': 0.4024, 'map_cut_1000': 0.2406}\n",
      "Dirichlet score for mu 2000 is {'P_5': 0.3933, 'recall_1000': 0.6302, 'ndcg_cut_10': 0.3998, 'map_cut_1000': 0.2406}\n"
     ]
    }
   ],
   "source": [
    "dirichlet_trec_scores = []\n",
    "for i, score in enumerate(dirichlet_scores):\n",
    "    trec_score = trec_eval.evaluate(data=score)\n",
    "    # Show score\n",
    "    print('Dirichlet score for mu {0} is {1}'.format(mus[i], trec_score))\n",
    "    trec_score['mu'] = mus[i]\n",
    "    dirichlet_trec_scores.append(trec_score)\n",
    "    \n",
    "# Save to not compute again\n",
    "dirichlet_file = 'dirichlet_validation.p'\n",
    "with open(dirichlet_file, 'wb') as fp:\n",
    "    pickle.dump(dirichlet_trec_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Dirichlet score using NDCG is 0.404 with mu of 1000\n"
     ]
    }
   ],
   "source": [
    "# Load and plot dirichlet scores for all parameter values\n",
    "# And select best model.\n",
    "mus = list(np.arange(500,2500,500))\n",
    "pos = {mu:position for position, mu in enumerate(mus)}\n",
    "\n",
    "# Fetch dirichlet scores\n",
    "with open('dirichlet_validation.p', 'rb') as fp:\n",
    "    dirichlet_all_scores = pickle.load(fp)\n",
    "\n",
    "# Get best mu.\n",
    "dirichlet_ndcg_10 = [0] * len(dirichlet_all_scores)\n",
    "\n",
    "for score in dirichlet_all_scores:\n",
    "    pos = mus.index(score['mu'])\n",
    "    dirichlet_ndcg_10[pos] = score['ndcg_cut_10']\n",
    "\n",
    "# Show best mu.\n",
    "best_ndcg = max(dirichlet_ndcg_10)\n",
    "best_mu = mus[dirichlet_ndcg_10.index(max(dirichlet_ndcg_10))]\n",
    "print('Best Dirichlet score using NDCG is {0} with mu of {1}'.format(best_ndcg, best_mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results of dirichlet optimized on validation set are: \n",
      "\n",
      "{'recall_1000': 0.6313, 'P_5': 0.4283, 'ndcg_cut_10': 0.4139, 'map_cut_1000': 0.2119}\n"
     ]
    }
   ],
   "source": [
    "# Recompute Dirichlet Prior module with best mu, and present results on test set.\n",
    "best_dirichlet_score = compute_measure(queries, measure_functions=[partial(dirichlet_score, mu=best_mu)], verbose=False)\n",
    "with open('best_dirichlet_scores.p', 'wb') as fp:\n",
    "    pickle.dump(best_dirichlet_score[0], fp)\n",
    "\n",
    "print('Test set results of dirichlet optimized on validation set are: \\n')\n",
    "print(trec_eval_test.evaluate(data=best_dirichlet_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset # Note: you need to rerun initial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 documents.\n",
      "Processed 20000 documents.\n",
      "Processed 30000 documents.\n",
      "Processed 40000 documents.\n",
      "Processed 50000 documents.\n",
      "Processed 60000 documents.\n",
      "Processed 70000 documents.\n",
      "Processed 80000 documents.\n",
      "Processed 90000 documents.\n",
      "Processed 100000 documents.\n",
      "Processed 110000 documents.\n",
      "Processed 120000 documents.\n",
      "Processed 130000 documents.\n",
      "Processed 140000 documents.\n",
      "Processed 150000 documents.\n",
      "Processed 160000 documents.\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Absolute Discounting\n",
    "deltas = np.arange(0.1,1.0,0.1)\n",
    "abs_funcs = [partial(abs_score, delta=delta_value) for delta_value in deltas]\n",
    "abs_scores = compute_measure(queries, measure_functions=abs_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Discounting score for delta 0.1 is {'P_5': 0.3533, 'recall_1000': 0.604, 'ndcg_cut_10': 0.3548, 'map_cut_1000': 0.2121}\n",
      "Absolute Discounting score for delta 0.2 is {'P_5': 0.3667, 'recall_1000': 0.6023, 'ndcg_cut_10': 0.3705, 'map_cut_1000': 0.2146}\n",
      "Absolute Discounting score for delta 0.30000000000000004 is {'P_5': 0.3933, 'recall_1000': 0.609, 'ndcg_cut_10': 0.3635, 'map_cut_1000': 0.2167}\n",
      "Absolute Discounting score for delta 0.4 is {'P_5': 0.3933, 'recall_1000': 0.6236, 'ndcg_cut_10': 0.3641, 'map_cut_1000': 0.2189}\n",
      "Absolute Discounting score for delta 0.5 is {'P_5': 0.4067, 'recall_1000': 0.6292, 'ndcg_cut_10': 0.3645, 'map_cut_1000': 0.2203}\n",
      "Absolute Discounting score for delta 0.6 is {'P_5': 0.4133, 'recall_1000': 0.6412, 'ndcg_cut_10': 0.3637, 'map_cut_1000': 0.2221}\n",
      "Absolute Discounting score for delta 0.7000000000000001 is {'P_5': 0.3933, 'recall_1000': 0.6504, 'ndcg_cut_10': 0.3673, 'map_cut_1000': 0.2234}\n",
      "Absolute Discounting score for delta 0.8 is {'P_5': 0.3867, 'recall_1000': 0.6585, 'ndcg_cut_10': 0.368, 'map_cut_1000': 0.2228}\n",
      "Absolute Discounting score for delta 0.9 is {'P_5': 0.3867, 'recall_1000': 0.6559, 'ndcg_cut_10': 0.3763, 'map_cut_1000': 0.2223}\n"
     ]
    }
   ],
   "source": [
    "abs_trec_scores = []\n",
    "for i, score in enumerate(abs_scores):\n",
    "    trec_score = trec_eval.evaluate(data=score)\n",
    "    # Show score\n",
    "    print('Absolute Discounting score for delta {0} is {1}'.format(deltas[i], trec_score))\n",
    "    trec_score['delta'] = deltas[i]\n",
    "    abs_trec_scores.append(trec_score)\n",
    "    \n",
    "# Save to not compute again\n",
    "abs_file = 'absdisc_validation.p'\n",
    "with open(abs_file, 'wb') as fp:\n",
    "    pickle.dump(abs_trec_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Abs Discounting score using NDCG is 0.3763 with delta of 0.9\n"
     ]
    }
   ],
   "source": [
    "# Load and plot Abs Discounting scores for all parameter values\n",
    "# And select best model.\n",
    "deltas = list(np.arange(0.1,1.0,0.1))\n",
    "pos = {delta:position for position, delta in enumerate(deltas)}\n",
    "\n",
    "# Fetch absolute discounting scores\n",
    "with open('absdisc_validation.p', 'rb') as fp:\n",
    "    absdisc_all_scores = pickle.load(fp)\n",
    "\n",
    "# Get best delta.\n",
    "absdisc_ndcg_10 = [0] * len(absdisc_all_scores)\n",
    "\n",
    "for score in absdisc_all_scores:\n",
    "    pos = deltas.index(score['delta'])\n",
    "    absdisc_ndcg_10[pos] = score['ndcg_cut_10']\n",
    "\n",
    "# Show best mu.\n",
    "best_ndcg = max(absdisc_ndcg_10)\n",
    "best_delta = deltas[absdisc_ndcg_10.index(max(absdisc_ndcg_10))]\n",
    "print('Best Abs Discounting score using NDCG is {0} with delta of {1}'.format(best_ndcg, best_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results of abs discounting optimized on validation set are: \n",
      "\n",
      "{'recall_1000': 0.6258, 'P_5': 0.4017, 'ndcg_cut_10': 0.396, 'map_cut_1000': 0.2037}\n"
     ]
    }
   ],
   "source": [
    "# Recompute Abs Disc. module with best mu, and present results on test set.\n",
    "best_absdisc_score = compute_measure(queries, measure_functions=[partial(abs_score, delta=best_delta)], verbose=False)\n",
    "with open('best_absdisc_scores.p', 'wb') as fp:\n",
    "    pickle.dump(best_absdisc_score[0], fp)\n",
    "\n",
    "print('Test set results of abs discounting optimized on validation set are: \\n')\n",
    "print(trec_eval_test.evaluate(data=best_absdisc_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAGDCAYAAABz1N6qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZm1KDcIzczMzMzMWtT/APAoXPPZRT0sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61a9699320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plots\n",
    "f = plt.figure()\n",
    "f.set_size_inches((15, 6))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "\n",
    "lamb = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "ax1.plot(lamb, jelinek_ndcg_10)\n",
    "\n",
    "ax1.grid(True)\n",
    "\n",
    "# plt.ylim((0.24, 0.34))\n",
    "plt.ylabel('NDCG')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Jelinek-Mercer')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "\n",
    "mus = np.arange(500,2500,500)\n",
    "\n",
    "ax2.plot(mus, dirichlet_ndcg_10)\n",
    "\n",
    "# plt.ylim((0.24, 0.34))\n",
    "plt.ylabel('NDCG')\n",
    "plt.xlabel('u')\n",
    "plt.title('Dirichlet Prior')\n",
    "ax2.grid(True)\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "\n",
    "delta = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "ax3.plot(delta, absdisc_ndcg_10)\n",
    "\n",
    "# plt.ylim((0.24, 0.34))\n",
    "plt.ylabel('NDCG')\n",
    "plt.xlabel('delta')\n",
    "plt.title('Absolute Discounting')\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Language Model\n",
    "\n",
    "- The main bottleneck of PLM is the computation of the Zi's, but as the author of the paper suggests, a Z_i can be calculated easily using an integration over the entire document space, which gives a form dependent on the cumulative distribution of each kernel function. Below are the final implementations of this integration and it's resultant form for each kernel, furthermore I also show that the sum computed normally is equal to the one computed using this implementation. \n",
    "\n",
    "**Note**: For the Gaussian kernel I use the Abramowitz and Stegun approximation of the cumulative normal distribution. This approximation is very good, as there seems to be only extremely minor differences. This does not affect ranking and can be seen after the implementations of the algorithm.\n",
    "\n",
    "Other optimizations performed: \n",
    "- I don't use the entire dataset, only the top 1k from the TF-IDF scoring, which will also help to compare as a baseline. This is motivated by the fact that I can still show a complete correct implementation of the model, but reduce the scale for computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triangle_kernel(i, j, sigma=50.0):\n",
    "    distance = abs(i - j)\n",
    "    return 1 - distance / sigma if distance <= sigma else 0\n",
    "\n",
    "def gaussian_kernel(i, j, sigma=50.0):\n",
    "    return np.exp(-(i - j) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "def cosine_kernel(i, j, sigma=50.0):\n",
    "    distance = abs(i - j)\n",
    "    return 0.5  + 0.5 * np.cos(distance * np.pi / sigma) if distance <= sigma else 0\n",
    "\n",
    "def circle_kernel(i, j, sigma=50.0):\n",
    "    distance = abs(i - j)\n",
    "    return np.sqrt(1 - (distance / sigma) ** 2) if distance <= sigma else 0\n",
    "\n",
    "def passage_kernel(i, j, sigma=50.0):\n",
    "    distance = abs(i - j)\n",
    "    return 1 if distance <= sigma else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cumulative distirbution functions for the kernels.\n",
    "\n",
    "def triangle_CDF(x, mean, sigma=50):\n",
    "    res = 0\n",
    "    x = (x - mean) / float(sigma)\n",
    "    if x >= 1:\n",
    "        res = sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (1 - abs(x)) * (1 - abs(x)) / 2.0\n",
    "    else:\n",
    "        res = sigma - sigma * (1 - x) * (1 - x) / 2.0\n",
    "    return res\n",
    "\n",
    "def gaussian_CDF(x, mean, sigma=50):\n",
    "    # Abramowitz and Stegun approximation.\n",
    "    res = 0.0\n",
    "    x = (x - mean) / float(sigma)\n",
    "    if x == 0:\n",
    "        res = 0.5\n",
    "    else:\n",
    "        oor_2_pi = 1.0 / (np.sqrt(2.0 * np.pi))\n",
    "        t = 1.0 / (1.0 + 0.2316419 * abs(x))\n",
    "        t *= oor_2_pi * np.exp(-0.5 * x * x) * (0.31938153   + t * (-0.356563782 + t * (1.781477937  + t * (-1.821255978 + t * 1.330274429))))\n",
    "        if x >= 0:\n",
    "            res = 1.0 - t\n",
    "        else:\n",
    "            res = t\n",
    "    return res\n",
    "\n",
    "def cosine_CDF(x, mean, sigma=50):\n",
    "    res = 0\n",
    "    x = (x - mean) / sigma\n",
    "    if x >= 1:\n",
    "        res = sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (1 + x - np.sin(np.pi * x) / np.pi) / 2.0\n",
    "    else:\n",
    "        res = sigma - sigma * (1 - x + np.sin(np.pi * x) / np.pi) / 2.0\n",
    "    \n",
    "    return res\n",
    "\n",
    "def circle_CDF(x, mean, sigma=50):\n",
    "    res = 0\n",
    "    x = (x - mean) / sigma\n",
    "    if x >= 1:\n",
    "        res = sigma * (np.pi / 2.0)\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (math.asin(x) + np.pi / 2.0 - np.sqrt(1 - x * x))\n",
    "    else:\n",
    "        res = (np.pi * 2.0) * sigma - sigma * (math.asin(-x) + np.pi / 2.0 - np.sqrt(1 - x * x))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These functions help compute Z_is directly using \n",
    "# cumulative distribution functions defined in previous cell.\n",
    "# (or their approximation - gaussian)\n",
    "# (and just a basic algo for the passage kernel)\n",
    "\n",
    "def triangle_normalization(position, doc_length, sigma):\n",
    "    return triangle_CDF(doc_length, position, sigma) - triangle_CDF(0.0, position, sigma)\n",
    "\n",
    "def gaussian_normalization(position, doc_length, sigma):\n",
    "    return np.sqrt(2.0 * np.pi) * sigma * (gaussian_CDF(doc_length-1, position, sigma) - gaussian_CDF(0.0, position, sigma))\n",
    "\n",
    "def cosine_normalization(position, doc_length, sigma):\n",
    "    return cosine_CDF(doc_length, position, sigma) - cosine_CDF(0.0, position, sigma)\n",
    "\n",
    "def circle_normalization(position, doc_length, sigma):\n",
    "    return circle_CDF(doc_length, position, sigma) - circle_CDF(0.0, position, sigma)\n",
    "\n",
    "def passage_normalization(position, doc_length, sigma=50):\n",
    "    psg = 0\n",
    "    if sigma > doc_length - position:\n",
    "        psg += doc_length - position\n",
    "    else:\n",
    "        psg += sigma\n",
    "        \n",
    "    if sigma > position:\n",
    "        psg += position\n",
    "    else:\n",
    "        if position > sigma:\n",
    "            psg += sigma + 1\n",
    "        else:\n",
    "            psg += sigma\n",
    "        \n",
    "    return psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian\n",
      "105.142650233\n",
      "105.446925755\n",
      "\n",
      "Cosine\n",
      "50.0\n",
      "50.0\n",
      "\n",
      "Circle\n",
      "78.4567127775\n",
      "78.5398163397\n",
      "\n",
      "Triangle\n",
      "50.0\n",
      "50.0\n",
      "\n",
      "Passage\n",
      "100\n",
      "100\n",
      "------\n",
      "Gaussian\n",
      "124.530996064\n",
      "124.553145777\n",
      "\n",
      "Cosine\n",
      "50.0\n",
      "50\n",
      "\n",
      "Circle\n",
      "78.4567127775\n",
      "78.53981633974483\n",
      "\n",
      "Triangle\n",
      "50.0\n",
      "50\n",
      "\n",
      "Passage\n",
      "101\n",
      "101\n",
      "------\n",
      "Gaussian\n",
      "125.331341817\n",
      "125.331337841\n",
      "\n",
      "Cosine\n",
      "50.0\n",
      "50\n",
      "\n",
      "Circle\n",
      "78.4567127775\n",
      "78.53981633974483\n",
      "\n",
      "Triangle\n",
      "50.0\n",
      "50\n",
      "\n",
      "Passage\n",
      "101\n",
      "101\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "## CDF normalization is done to compute Z_i's faster.\n",
    "## Testing some values for correctness:\n",
    "\n",
    "sigma = 50\n",
    "positions = [50, 125, 250]\n",
    "doc_length = 500\n",
    "\n",
    "for pos in positions:\n",
    "    print('Gaussian')\n",
    "    print(sum([gaussian_kernel(pos,j,sigma) for j in range(1, doc_length + 1)]))\n",
    "    print(gaussian_normalization(pos, doc_length, sigma))\n",
    "\n",
    "    print('\\nCosine')\n",
    "    print(sum([cosine_kernel(pos,j,sigma) for j in range(1, doc_length + 1)]))\n",
    "    print(cosine_normalization(pos, doc_length, sigma))\n",
    "    \n",
    "    print('\\nCircle')\n",
    "    print(sum([circle_kernel(pos,j,sigma) for j in range(1, doc_length + 1)]))\n",
    "    print(circle_normalization(pos, doc_length, sigma))\n",
    "\n",
    "    print('\\nTriangle')\n",
    "    print(sum([triangle_kernel(pos,j,sigma) for j in range(1, doc_length + 1)]))\n",
    "    print(triangle_normalization(pos, doc_length, sigma))\n",
    "\n",
    "    print('\\nPassage')\n",
    "    print(sum([passage_kernel(pos,j,sigma) for j in range(1, doc_length + 1)]))\n",
    "    print(passage_normalization(pos, doc_length, sigma))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute all kernel operations beforehand\n",
    "# by computing a kernel matrix of the maximum document length\n",
    "# for each kernel operation. This ensures that now any k(i,j) is O(1)\n",
    "g_matrix = np.zeros((longest_doc_length,longest_doc_length))\n",
    "circle_matrix = np.zeros((longest_doc_length,longest_doc_length))\n",
    "passage_matrix = np.zeros((longest_doc_length,longest_doc_length))\n",
    "cosine_matrix = np.zeros((longest_doc_length,longest_doc_length))\n",
    "triangle_matrix = np.zeros((longest_doc_length,longest_doc_length))\n",
    "\n",
    "for x in range(longest_doc_length):\n",
    "    for y in range(longest_doc_length):\n",
    "        g_matrix[x][y] = gaussian_kernel(x,y)\n",
    "        circle_matrix[x][y] = circle_kernel(x,y)\n",
    "        passage_matrix[x][y] = passage_kernel(x,y)\n",
    "        cosine_matrix[x][y] = cosine_kernel(x,y)\n",
    "        triangle_matrix[x][y] = triangle_kernel(x,y)\n",
    "        \n",
    "# Note: Later I realized that if you do a blocksize sum (G[i,0:N]), you can actually compute Zi using the matrix,\n",
    "# but I had already done some of the integrations for the CDF so I will use that method. \n",
    "# performance should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_tfidf_res(top_k=1000, get_docs=True):    \n",
    "    \"\"\"\n",
    "    Load TF-IDF query-documents to rerank using another model\n",
    "    Only load first top_k documents for eacy query, where top_k can be\n",
    "    max 1000.\n",
    "    \n",
    "    If get_docs=True returns a mapping from the top_k documents\n",
    "    to a list of queries where to document is in the top_k of that query.\n",
    "    i.e. 'doc_id' -> [queries]\n",
    "    \n",
    "    If get_docs=False, returns a mapping of query to top documents. But\n",
    "    returns only doc_ids and not their scores.\n",
    "    \"\"\"\n",
    "    with open(tf_file, 'rb') as fp:\n",
    "        tf_idf_results = pickle.load(fp)\n",
    "    \n",
    "    ranked_list = OrderedDict()\n",
    "    if get_docs:\n",
    "        for query_id in tf_idf_results:\n",
    "            docs = tf_idf_results[query_id][:top_k]\n",
    "            docs = index.document_ids([doc[1] for doc in docs])\n",
    "            for _, doc_id in docs:\n",
    "                if not doc_id in ranked_list:\n",
    "                    ranked_list[doc_id] = [query_id]\n",
    "                else:\n",
    "                    ranked_list[doc_id].append(query_id)\n",
    "    else:\n",
    "        for query_id in tf_idf_results:\n",
    "            docs = tf_idf_results[query_id][:top_k]\n",
    "            docs = index.document_ids([doc[1] for doc in docs])\n",
    "            \n",
    "            # Save only the document identifier.\n",
    "            ranked_list[query_id] = [doc[1] for doc in docs] \n",
    "            \n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_docs = load_tfidf_res(top_k=400)\n",
    "print(len(ranked_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_process_doc(document):\n",
    "    \"\"\"\n",
    "    Process a document transforming every term into\n",
    "    a dictionary key with it's positions in the document as values.\n",
    "    \"\"\"\n",
    "    # Save positions of every term.\n",
    "    doc_words = {}\n",
    "    for pos, term in enumerate(document[1]):\n",
    "        if term in doc_words:\n",
    "            doc_words[term].append(pos)\n",
    "        else:\n",
    "            doc_words[term] = []\n",
    "    return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "## Implementation of PLM model\n",
    "def plm_score(document, doc_words, doc_zcache, query, mu=500, sigma=50):\n",
    "    \"\"\"\n",
    "    Positional language model scoring given a document and a query.\n",
    "    Several optimizations are made, most explained in the paper.\n",
    "    One obvious one is, given a document where query words are denoted with q\n",
    "\n",
    "    -----------q--------------q---------------------------q-----------\n",
    "    \n",
    "    We cut the 'tails' of the document to start from the left and right q,\n",
    "    as biggest PLM will always be within that center.\n",
    "    \n",
    "    Uses CDF functions for Zi computation. Computes all 5 kernel scores.\n",
    "    \"\"\"\n",
    "    biggest_score = [-99999999,-99999999,-99999999,-99999999,-99999999]\n",
    "    doc_length = len(document[1])\n",
    "    query_length = query['length']\n",
    "    \n",
    "    # Query likelihood model\n",
    "    qlm = 1.0 / query_length\n",
    "    \n",
    "    first, last = 0, doc_length-1\n",
    "    while document[1][first] not in query['terms'] and document[1][last] not in query['terms']:\n",
    "        if document[1][first] not in query['terms']:\n",
    "            first += 1\n",
    "        \n",
    "        if document[1][last] not in query['terms']:\n",
    "            last -= 1\n",
    "\n",
    "    # For every position in the document\n",
    "    # compute a PLM\n",
    "    for i in range(first, last+1):\n",
    "        # i is the position of current PLM\n",
    "        # Get Z_i for the current position\n",
    "        # Use memoization\n",
    "        if i not in doc_zcache:\n",
    "            doc_zcache[i] = [gaussian_normalization(position=i, doc_length=doc_length, sigma=sigma),\n",
    "                             circle_normalization(position=i, doc_length=doc_length, sigma=sigma),\n",
    "                             passage_normalization(position=i, doc_length=doc_length, sigma=sigma),\n",
    "                             cosine_normalization(position=i, doc_length=doc_length, sigma=sigma),\n",
    "                            triangle_normalization(position=i, doc_length=doc_length, sigma=sigma)]\n",
    "                        \n",
    "        # Initialize propagated count for all words in the query.\n",
    "        # For all kernels\n",
    "        propagated_count = np.zeros((5, query_length)) \n",
    "\n",
    "        # Get propagated count for each word in the query\n",
    "        for word_index, query_term in enumerate(query['terms']):\n",
    "            # Find occurances of the query word in the document\n",
    "            if query_term in doc_words:\n",
    "                for j in doc_words[query_term]:\n",
    "                    propagated_count[0, word_index] += g_matrix[i,j]\n",
    "                    propagated_count[1, word_index] += circle_matrix[i,j]\n",
    "                    propagated_count[2, word_index] += passage_matrix[i,j]\n",
    "                    propagated_count[3, word_index] += cosine_matrix[i,j]\n",
    "                    propagated_count[4, word_index] += triangle_matrix[i,j]\n",
    "\n",
    "            # Finished going through document.\n",
    "            # Aply dirichlet prior smoothing\n",
    "            # i.e., now we have c'(w,i), compute the fraction\n",
    "            propagated_count[:, word_index] += mu * id2colprob[query_term]\n",
    "\n",
    "        # Apply normalization to all propagated counts.\n",
    "        # Divide by Z_i + mu\n",
    "        for c in range(5):\n",
    "            propagated_count[c,:] /= (doc_zcache[i][c] + mu) \n",
    "\n",
    "        # KL-DIV, a.k.a. the score S(Q,D,i) of the PLM \n",
    "        sqd = [0,0,0,0,0]\n",
    "        for s in range(5):\n",
    "            for plm_sc in propagated_count[s]:\n",
    "                sqd[s] += -qlm * math.log(qlm / plm_sc)\n",
    "\n",
    "        # Use biggest score method to get score of document\n",
    "        for sc in range(5):\n",
    "            if biggest_score[sc] < sqd[sc]:\n",
    "                biggest_score[sc] = sqd[sc]\n",
    "                \n",
    "    # Return all 5 scores\n",
    "    return biggest_score\n",
    "\n",
    "def rank_plm(ranked_docs, all_queries, mu=500, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a set of scored queries (in the form of a dictionary),\n",
    "    reranks their documents, according to the PLM using all 5 kernels.\n",
    "    \"\"\"\n",
    "    scores = [defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)]\n",
    "    counter = 0\n",
    "    for doc_id, doc_queries in ranked_docs.items():\n",
    "        counter += 1\n",
    "        if verbose and counter % 5000 == 0:\n",
    "            print(counter)\n",
    "        # Current document to get score for\n",
    "        curr_doc = index.document(doc_id)\n",
    "\n",
    "        # Disconsider documents with length < 400\n",
    "        if len(curr_doc[1]) < 400:\n",
    "            continue\n",
    "\n",
    "        # 5x speedup here.\n",
    "        # Store positions where words occur.\n",
    "        doc_words = pos_process_doc(curr_doc)\n",
    "\n",
    "        # Memoization of Zis\n",
    "        # Cache for normalization parameters.\n",
    "        doc_cache = {}\n",
    "\n",
    "        for query_id in doc_queries:\n",
    "            query_info = all_queries[query_id]\n",
    "\n",
    "            pair_scores = plm_score(document=curr_doc, doc_words=doc_words, doc_zcache=doc_cache, query=query_info, mu=mu, sigma=50)        \n",
    "\n",
    "            for sc in range(len(pair_scores)):\n",
    "                scores[sc][query_id].append((pair_scores[sc], curr_doc[0]))\n",
    "\n",
    "    for score in scores:\n",
    "        for query_id in score:\n",
    "            score[query_id] = heapq.nlargest(400, score[query_id], itemgetter(0))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "mu_500_scores = rank_plm(ranked_docs=ranked_docs, all_queries=queries, mu=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu500_file = 'mu_500_scores.p'\n",
    "with open(mu500_file, 'wb') as fp:\n",
    "    pickle.dump(mu_500_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scores for each kernel using mu=1000\n",
    "mu_1000_scores = rank_plm(ranked_docs=ranked_docs, all_queries=queries, mu=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu1000_file = 'mu_1000_scores.p'\n",
    "with open(mu1000_file, 'wb') as fp:\n",
    "    pickle.dump(mu_1000_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scores for each kernel using mu=1000\n",
    "mu_1500_scores = rank_plm(ranked_docs=ranked_docs, all_queries=queries, mu=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu1500_file = 'mu_1500_scores.p'\n",
    "with open(mu1500_file, 'wb') as fp:\n",
    "    pickle.dump(mu_1500_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_2000_scores = rank_plm(ranked_docs=ranked_docs, all_queries=queries, mu=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu2000_file = 'mu_2000_scores.p'\n",
    "with open(mu2000_file, 'wb') as fp:\n",
    "    pickle.dump(mu_2000_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation set using mu=500 and kernel=gaussian are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.3521, 'map_cut_1000': 0.1415}\n",
      "Results on validation set using mu=500 and kernel=circle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3733, 'ndcg_cut_10': 0.3547, 'map_cut_1000': 0.1367}\n",
      "Results on validation set using mu=500 and kernel=passage are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3533, 'ndcg_cut_10': 0.3488, 'map_cut_1000': 0.1388}\n",
      "Results on validation set using mu=500 and kernel=cosine are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.3488, 'map_cut_1000': 0.1396}\n",
      "Results on validation set using mu=500 and kernel=triangle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.346, 'map_cut_1000': 0.1393}\n"
     ]
    }
   ],
   "source": [
    "# Determine best score calculated using Positiona Language Model\n",
    "prop_score = ['gaussian', 'circle', 'passage', 'cosine', 'triangle']\n",
    "\n",
    "# Scores for mu=500, using all 5 kernels.\n",
    "with open('mu_500_scores.p', 'rb') as fp:\n",
    "    mu_500 = pickle.load(fp)\n",
    "    \n",
    "for i, score in enumerate(mu_500):\n",
    "    print('Results on validation set using mu={0} and kernel={1} are: \\n {2}'.format(500, prop_score[i],trec_eval.evaluate(data=score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation set using mu=1000 and kernel=gaussian are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.3529, 'map_cut_1000': 0.1422}\n",
      "Results on validation set using mu=1000 and kernel=circle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3533, 'ndcg_cut_10': 0.3453, 'map_cut_1000': 0.1423}\n",
      "Results on validation set using mu=1000 and kernel=passage are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.36, 'ndcg_cut_10': 0.3522, 'map_cut_1000': 0.1398}\n",
      "Results on validation set using mu=1000 and kernel=cosine are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.36, 'ndcg_cut_10': 0.3378, 'map_cut_1000': 0.134}\n",
      "Results on validation set using mu=1000 and kernel=triangle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.3382, 'map_cut_1000': 0.1337}\n"
     ]
    }
   ],
   "source": [
    "# Scores for mu=1000, using all 5 kernels.\n",
    "with open('mu_1000_scores.p', 'rb') as fp:\n",
    "    mu_1000 = pickle.load(fp)\n",
    "    \n",
    "for i, score in enumerate(mu_1000):\n",
    "    print('Results on validation set using mu={0} and kernel={1} are: \\n {2}'.format(1000, prop_score[i],trec_eval.evaluate(data=score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation set using mu=1500 and kernel=gaussian are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.36, 'ndcg_cut_10': 0.3544, 'map_cut_1000': 0.1415}\n",
      "Results on validation set using mu=1500 and kernel=circle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3533, 'ndcg_cut_10': 0.3472, 'map_cut_1000': 0.1427}\n",
      "Results on validation set using mu=1500 and kernel=passage are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.36, 'ndcg_cut_10': 0.3548, 'map_cut_1000': 0.1403}\n",
      "Results on validation set using mu=1500 and kernel=cosine are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.36, 'ndcg_cut_10': 0.3337, 'map_cut_1000': 0.1324}\n",
      "Results on validation set using mu=1500 and kernel=triangle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.3391, 'map_cut_1000': 0.1345}\n"
     ]
    }
   ],
   "source": [
    "# Scores for mu=1500, using all 5 kernels.\n",
    "with open('mu_1500_scores.p', 'rb') as fp:\n",
    "    mu_1500 = pickle.load(fp)\n",
    "    \n",
    "for i, score in enumerate(mu_1500):\n",
    "    print('Results on validation set using mu={0} and kernel={1} are: \\n {2}'.format(1500, prop_score[i],trec_eval.evaluate(data=score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation set using mu=2000 and kernel=gaussian are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3467, 'ndcg_cut_10': 0.3529, 'map_cut_1000': 0.141}\n",
      "Results on validation set using mu=2000 and kernel=circle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3533, 'ndcg_cut_10': 0.3459, 'map_cut_1000': 0.1423}\n",
      "Results on validation set using mu=2000 and kernel=passage are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3533, 'ndcg_cut_10': 0.3516, 'map_cut_1000': 0.1403}\n",
      "Results on validation set using mu=2000 and kernel=cosine are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3533, 'ndcg_cut_10': 0.335, 'map_cut_1000': 0.1324}\n",
      "Results on validation set using mu=2000 and kernel=triangle are: \n",
      " {'recall_1000': 0.3543, 'P_5': 0.3667, 'ndcg_cut_10': 0.3365, 'map_cut_1000': 0.1346}\n"
     ]
    }
   ],
   "source": [
    "# Scores for mu=2000, using all 5 kernels.\n",
    "with open('mu_2000_scores.p', 'rb') as fp:\n",
    "    mu_2000 = pickle.load(fp)\n",
    "    \n",
    "for i, score in enumerate(mu_2000):\n",
    "    print('Results on validation set using mu={0} and kernel={1} are: \\n {2}'.format(2000, prop_score[i],trec_eval.evaluate(data=score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set result of best plm is {'recall_1000': 0.3708, 'P_5': 0.3733, 'ndcg_cut_10': 0.3708, 'map_cut_1000': 0.137}\n"
     ]
    }
   ],
   "source": [
    "# Note: Keep in mind this is a reranking of the tfidf ranking.\n",
    "# There are no major differences in terms of the results between the mus/kernls.\n",
    "# By observatian the gaussian kernel seems to outperform the others slightly considering NDCG\n",
    "# So going forward we will use -> The score of the gaussian kernel, with mu=1000\n",
    "\n",
    "best_plm = mu_1000[0] # Gaussian kernel\n",
    "\n",
    "print('Test set result of best plm is {0}'.format(trec_eval_test.evaluate(data=best_plm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def process_model_results(results):\n",
    "    \"\"\"\n",
    "    Given trec eval complete results for all query document pairs.\n",
    "    Returns 4 arrays containing the respective measure values.\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    recall_scores = []\n",
    "    p5_scores = []\n",
    "    map_scores = []\n",
    "\n",
    "    for query_id, query_scores in results.items():\n",
    "        if query_id == 'all':\n",
    "            continue\n",
    "        p5_scores.append(query_scores[0]['P_5'])\n",
    "        recall_scores.append(query_scores[1]['recall_1000'])\n",
    "        ndcg_scores.append(query_scores[2]['ndcg_cut_10'])\n",
    "        map_scores.append(query_scores[3]['map_cut_1000'])\n",
    "    \n",
    "    return p5_scores, recall_scores, ndcg_scores, map_scores\n",
    "        \n",
    "def compute_related_ttest(scoresA, scoresB):\n",
    "    \"\"\"\n",
    "    Computes two-tailed paired Student t-test, for all \n",
    "    pairs of scores given the 4 measures.\n",
    "    \n",
    "    Uses sidak correction\n",
    "    https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction\n",
    "    \"\"\"\n",
    "    measure_names = ['Precision@5', 'Recall@1000','NDCG@10','MAP@1000']\n",
    "    alpha = 0.05\n",
    "    comparisons = 4\n",
    "    alpha_sid = 1 - (1 - alpha)**0.25 # 1/4\n",
    "    \n",
    "    # Get results for both scores\n",
    "    resA = trec_eval_test.evaluate(data=scoresA, complete=True)\n",
    "    resB = trec_eval_test.evaluate(data=scoresB, complete=True)\n",
    "    \n",
    "    # Unpack results of all measures\n",
    "    all_scA = process_model_results(resA)\n",
    "    all_scB = process_model_results(resB)\n",
    "    \n",
    "    for i in range(comparisons):\n",
    "        t, p_value = ttest_rel(all_scA[i],all_scB[i])\n",
    "        if p_value < alpha_sid:\n",
    "            print('Null hypothesis of equal averages rejected for {0}'.format(measure_names[i]))\n",
    "        else:\n",
    "            print('Null hypothesis of equal averages accepted for {0}'.format(measure_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest for comparisons between BM25 and TF-IDF\n",
      "Null hypothesis of equal averages rejected for Precision@5\n",
      "Null hypothesis of equal averages rejected for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n"
     ]
    }
   ],
   "source": [
    "### Perform some statistical significance tests\n",
    "print('Ttest for comparisons between BM25 and TF-IDF')\n",
    "with open(tf_file, 'rb') as fp:\n",
    "    tf = pickle.load(fp)\n",
    "    \n",
    "with open(bm25_file, 'rb') as fp:\n",
    "    bm25 = pickle.load(fp)\n",
    "    \n",
    "compute_related_ttest(tf, bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Absolute Discounting and TF-IDF\n",
      "Null hypothesis of equal averages accepted for Precision@5\n",
      "Null hypothesis of equal averages accepted for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n",
      "Comparison between Dirichlet and TF-IDF\n",
      "Null hypothesis of equal averages rejected for Precision@5\n",
      "Null hypothesis of equal averages rejected for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n",
      "Comparison between  Jelinek and TF-IDF\n",
      "Null hypothesis of equal averages accepted for Precision@5\n",
      "Null hypothesis of equal averages rejected for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n",
      "Comparison between PLM Reranking of TF-IDF and TF-IDF\n",
      "Null hypothesis of equal averages accepted for Precision@5\n",
      "Null hypothesis of equal averages rejected for Recall@1000\n",
      "Null hypothesis of equal averages accepted for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n"
     ]
    }
   ],
   "source": [
    "## Compare TF-IDF and best Jalinek, Dirichlet, ABS.DISC, PLM\n",
    "abs_disc = best_absdisc_score[0]\n",
    "dirichlet = best_dirichlet_score[0]\n",
    "jalinek = best_jelinek_score[0]\n",
    "\n",
    "print('Comparison between Absolute Discounting and TF-IDF\\n')\n",
    "compute_related_ttest(tf, abs_disc)\n",
    "\n",
    "print('Comparison between Dirichlet and TF-IDF\\n')\n",
    "compute_related_ttest(tf, dirichlet)\n",
    "\n",
    "print('Comparison between  Jelinek and TF-IDF\\n')\n",
    "compute_related_ttest(tf, jalinek)\n",
    "\n",
    "print('Comparison between PLM Reranking of TF-IDF and TF-IDF')\n",
    "compute_related_ttest(tf, best_plm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [25 points + 10 bonus points] ###\n",
    "\n",
    "In this task you will experiment with applying a distributional semantics methods ([word2vec](http://arxiv.org/abs/1411.2738)  **[5 points]**, [LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]**, [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]** and [doc2vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement word2vec, LSI, LDA and doc2vec on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html) (pre-loaded on the VirtualBox). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. For example, in the case of word2vec, you only have vectors for individual words and not for documents or phrases. Try one of the following methods for producing these representations:\n",
    "   * Average or sum the word vectors.\n",
    "   * Cluster words in the document using [k-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and use the centroid of the most important cluster. Experiment with different values of K for k-means.\n",
    "   * Using the [bag-of-word-embeddings representation](https://ciir-publications.cs.umass.edu/pub/web/getpdf.php?id=1248). **[10 bonus points]**\n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# File names and constants\n",
    "# File to save word2vec model\n",
    "from gensim import models\n",
    "\n",
    "DIM_LSI = 64\n",
    "DIM_LDA = 32\n",
    "DIM_HIDDEN = 32\n",
    "\n",
    "word2vec_file = 'word2vec_model.bin'\n",
    "doc2vec_file = 'doc2vec_model.doc2vec'\n",
    "\n",
    "corpus_file = 'pyndri_corpus.mm'\n",
    "dict_file='corpdict.dict'\n",
    "\n",
    "\n",
    "tfidf_model_file = 'tfidf_model.model'\n",
    "lsi_model_file = 'lsi_model.model'\n",
    "tfidf_corpora_file = 'tfidf_corpora.mm'\n",
    "lsi_corpora_file = 'lsi_corpora.mm'\n",
    "\n",
    "lda_model_file = 'lda_model.model'\n",
    "lda_corpora_file = 'lda_corpora.mm'\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "available_cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing word2vec.\n",
      "INFO:gensim.models.word2vec:loading projection weights from word2vec_model.bin\n",
      "INFO:gensim.models.word2vec:loaded (102844, 32) matrix from word2vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "logging.info('Initializing word2vec.')\n",
    "\n",
    "# Get dictionary and sentences\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "if not os.path.isfile(word2vec_file):\n",
    "    # Build model, train for default 5 iterations.\n",
    "    # Use size of 32 for hidden layer for speed.\n",
    "    model = gensim.models.Word2Vec(sentences, \n",
    "                                   size=DIM_HIDDEN, \n",
    "                                   window=5, \n",
    "                                   min_count=5, \n",
    "                                   workers=available_cores, # Set maximum cores.\n",
    "                                   sample=1e-3, \n",
    "                                   negative=5, \n",
    "                                   sg=True)\n",
    "    model.save_word2vec_format(word2vec_file, binary=True)\n",
    "else:\n",
    "    # Model already trained, load it.\n",
    "    word2vec_model = gensim.models.Word2Vec.load_word2vec_format(word2vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Doc2Vec object from doc2vec_model.doc2vec\n",
      "INFO:gensim.utils:loading docvecs recursively from doc2vec_model.doc2vec.docvecs.* with mmap=None\n",
      "INFO:gensim.utils:loading wv recursively from doc2vec_model.doc2vec.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute syn0norm to None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:loaded doc2vec_model.doc2vec\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(doc2vec_file):\n",
    "    from collections import namedtuple\n",
    "    # Get dictionary and sentences\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "    # Process data for doc2vec model\n",
    "    # Don't really need a 'connector class' here.\n",
    "    doc_model = namedtuple('document', 'words tags')\n",
    "\n",
    "    docs = []\n",
    "    curr_doc = 0\n",
    "    for sent in sentences:\n",
    "        curr_doc += 1\n",
    "        tags = [curr_doc]\n",
    "        docs.append(doc_model(list(sent), tags))\n",
    "        \n",
    "    # Train model\n",
    "    assert gensim.models.doc2vec.FAST_VERSION \n",
    "    model = gensim.models.Doc2Vec(docs, \n",
    "                                  size=DIM_HIDDEN, \n",
    "                                  window=5, \n",
    "                                  min_count=5, \n",
    "                                  workers=available_cores)\n",
    "    model.save(doc2vec_file)\n",
    "else:\n",
    "    doc2vec_model = models.Doc2Vec.load(doc2vec_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI\n",
    "\n",
    "Note: https://github.com/RaRe-Technologies/gensim/commit/e4114336591e2dcd311260b7b221e2427335417a#diff-67d50c48e7c1e120919ef6faf7097311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Dictionary object from corpdict.dict\n",
      "INFO:gensim.utils:loaded corpdict.dict\n",
      "INFO:gensim.corpora.indexedcorpus:loaded corpus index from pyndri_corpus.mm.index\n",
      "INFO:gensim.matutils:initializing corpus reader from pyndri_corpus.mm\n",
      "INFO:gensim.matutils:accepted corpus with 164597 documents, 85926 features, 29285443 non-zero entries\n",
      "INFO:gensim.corpora.indexedcorpus:loaded corpus index from tfidf_corpora.mm.index\n",
      "INFO:gensim.matutils:initializing corpus reader from tfidf_corpora.mm\n",
      "INFO:gensim.matutils:accepted corpus with 164597 documents, 85926 features, 29285443 non-zero entries\n",
      "INFO:gensim.corpora.indexedcorpus:loaded corpus index from lsi_corpora.mm.index\n",
      "INFO:gensim.matutils:initializing corpus reader from lsi_corpora.mm\n",
      "INFO:gensim.matutils:accepted corpus with 164597 documents, 64 features, 10533504 non-zero entries\n",
      "INFO:gensim.utils:loading TfidfModel object from tfidf_model.model\n",
      "INFO:gensim.utils:loading id2word recursively from tfidf_model.model.id2word.* with mmap=None\n",
      "INFO:gensim.utils:loaded tfidf_model.model\n",
      "INFO:gensim.utils:loading LsiModel object from lsi_model.model\n",
      "INFO:gensim.utils:loading id2word recursively from lsi_model.model.id2word.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute projection to None\n",
      "INFO:gensim.utils:setting ignored attribute dispatcher to None\n",
      "INFO:gensim.utils:loaded lsi_model.model\n",
      "INFO:gensim.utils:loading LsiModel object from lsi_model.model.projection\n",
      "INFO:gensim.utils:loaded lsi_model.model.projection\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import lsimodel \n",
    "from gensim.models import tfidfmodel\n",
    "\n",
    "# Use indrisentences \n",
    "def get_pyndri_sentences(index):\n",
    "    # Get iterator to pyndri sentences\n",
    "    pyindri_dict = pyndri.extract_dictionary(index)\n",
    "    docs = pyndri.compat.IndriSentences(index, dictionary)\n",
    "    return docs\n",
    "\n",
    "# 'Connect' class.\n",
    "class PyndriCorpus(object):\n",
    "    \n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        \n",
    "        # Construct and filter corpora dictionary\n",
    "        self.dictionary = corpora.Dictionary(get_pyndri_sentences(index))\n",
    "        self.dictionary.filter_extremes(no_below=5)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for doc in get_pyndri_sentences(index):\n",
    "            yield self.dictionary.doc2bow(doc)\n",
    "\n",
    "    def save_dict(self):\n",
    "        self.dictionary.save(dict_file)\n",
    "        \n",
    "if not os.path.isfile(corpus_file):\n",
    "    # Construct and serialize the corpus\n",
    "    corpus = PyndriCorpus(index)\n",
    "    corpora.MmCorpus.serialize(corpus_file, corpus)\n",
    "    corpus.save_dict()\n",
    "else:  \n",
    "    # Load corpus from disk\n",
    "    dictionary = corpora.Dictionary.load(dict_file)\n",
    "    corpus = corpora.MmCorpus(corpus_file)\n",
    "    \n",
    "if not os.path.isfile(lsi_model_file) or not os.path.isfile(tfidf_model_file):\n",
    "    # first, transform word counts to tf-idf weights\n",
    "    tfidf_model = tfidfmodel.TfidfModel(corpus, id2word = dictionary, normalize = True)\n",
    "\n",
    "    # then find the transformation from tf-idf to latent space\n",
    "    lsi_model = lsimodel.LsiModel(tfidf_model[corpus], id2word = dictionary, num_topics = DIM_LSI)\n",
    "    \n",
    "    # Transformed_corpora\n",
    "    tfidf_corpora = tfidf_model[corpus]\n",
    "    lsi_corpora = lsi_model[tfidf_model[corpus]]\n",
    "    \n",
    "    # Cache the transformed corpora to disk\n",
    "    corpora.MmCorpus.serialize(tfidf_corpora_file, tfidf_corpora)\n",
    "    corpora.MmCorpus.serialize(lsi_corpora_file, lsi_corpora)\n",
    "\n",
    "    # Cache the models to disk\n",
    "    tfidf_model.save(tfidf_model_file)\n",
    "    lsi_model.save(lsi_model_file)\n",
    "else:\n",
    "    # Load corpora and models\n",
    "    tfidf_corpora = gensim.corpora.MmCorpus(tfidf_corpora_file)\n",
    "    lsi_corpora = gensim.corpora.MmCorpus(lsi_corpora_file)\n",
    "\n",
    "    tfidf_model = gensim.models.TfidfModel.load(tfidf_model_file)\n",
    "    lsi_model = gensim.models.LsiModel.load(lsi_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Dictionary object from corpdict.dict\n",
      "INFO:gensim.utils:loaded corpdict.dict\n",
      "INFO:gensim.corpora.indexedcorpus:loaded corpus index from pyndri_corpus.mm.index\n",
      "INFO:gensim.matutils:initializing corpus reader from pyndri_corpus.mm\n",
      "INFO:gensim.matutils:accepted corpus with 164597 documents, 85926 features, 29285443 non-zero entries\n",
      "INFO:gensim.utils:loading LdaMulticore object from lda_model.model\n",
      "INFO:gensim.utils:loading expElogbeta from lda_model.model.expElogbeta.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute id2word to None\n",
      "INFO:gensim.utils:setting ignored attribute state to None\n",
      "INFO:gensim.utils:setting ignored attribute dispatcher to None\n",
      "INFO:gensim.utils:loaded lda_model.model\n",
      "INFO:gensim.utils:loading LdaMulticore object from lda_model.model.state\n",
      "INFO:gensim.utils:loaded lda_model.model.state\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "if not os.path.isfile(corpus_file):\n",
    "    # Construct and serialize the corpus\n",
    "    corpus = PyndriCorpus(index)\n",
    "    corpora.MmCorpus.serialize(corpus_file, corpus)\n",
    "    corpus.save_dict()\n",
    "    dictionary = corpus.dictionary\n",
    "else:\n",
    "    # Load corpus from disk\n",
    "    dictionary = corpora.Dictionary.load(dict_file)\n",
    "    corpus = corpora.MmCorpus(corpus_file)\n",
    "    \n",
    "if not os.path.isfile(lda_model_file):\n",
    "    # Multicore implementation of LDA\n",
    "    lda_model = LdaMulticore(corpus, num_topics=DIM_LDA, id2word=dictionary, workers=available_cores, passes=2)\n",
    "    lda_model.save(lda_model_file)\n",
    "    \n",
    "    lda_corpora = lda_model[corpus]\n",
    "    # Cache the transformed corpora to disk\n",
    "    corpora.MmCorpus.serialize(lda_corpora_file, lda_corpora)\n",
    "else:\n",
    "    lda_model = LdaMulticore.load(lda_model_file)\n",
    "    lsi_corpora = gensim.corpora.MmCorpus(lda_corpora_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluations\n",
    "\n",
    "- We use the representations constructed by the latent semantic models, to rerank the top 1k documents from the tf_idf scoring in Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform each query and each unique document in a representation.\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "from sklearn.metrics.cluster import mutual_info_score as kl_div\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# First fetch tf_idf results.\n",
    "tfidf_queries = load_tfidf_res(top_k=1000, get_docs=False)\n",
    "raw_queries = preprocess_queries(raw_terms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_score(ranked_queries, all_queries, word2vec_model):\n",
    "    \"\"\"\n",
    "    Given a mapping from query_id -> list of top documents \n",
    "    return from another retrieval algorithm, reranks the documents\n",
    "    using the word2vec embedding representation and cosine similarity.\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    for query_id, top_docs in ranked_queries.items():\n",
    "        # Construct query representation\n",
    "        query_repr = np.mean(word2vec_model[raw_queries[query_id]['raw_terms']], axis=0)\n",
    "        \n",
    "        # For each document\n",
    "        for doc_id in ranked_queries[query_id]:\n",
    "            # Get document and words\n",
    "            current_doc = index.document(doc_id)\n",
    "            doc_words = [id2token[token_id] for token_id in current_doc[1] if token_id > 0 and \n",
    "                         id2colfreq[token_id] > 5 and token_id in id2token]\n",
    "            \n",
    "            # Construct document representation\n",
    "            doc_repr = np.mean(word2vec_model[doc_words], axis=0)\n",
    "            \n",
    "            # Compute similarity score.\n",
    "            sim = cos_sim(np.reshape(query_repr, (1, -1)), np.reshape(doc_repr, (1, -1))) \n",
    "            \n",
    "            # Store similarity\n",
    "            scores[query_id].append((sim.item(), current_doc[0]))\n",
    "    \n",
    "    for query_id in scores:\n",
    "            scores[query_id] = heapq.nlargest(1000, scores[query_id], itemgetter(0))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_rerankings = word2vec_score(ranked_queries=tfidf_queries, all_queries=raw_queries, word2vec_model=word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_scores_file = 'w2vec_scores.p'\n",
    "with open(word2vec_scores_file, 'wb') as fp:\n",
    "    pickle.dump(word2vec_rerankings, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of TF-IDF reranking using word2vec is {'recall_1000': 0.6089, 'P_5': 0.3083, 'ndcg_cut_10': 0.3113, 'map_cut_1000': 0.1601}\n"
     ]
    }
   ],
   "source": [
    "## Use on test set\n",
    "print('Result of TF-IDF reranking using word2vec is {0}'.format(trec_eval_test.evaluate(word2vec_rerankings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc2vec_score(ranked_queries, all_queries, doc2vec_model):\n",
    "    \"\"\"\n",
    "    Given a mapping from query_id -> list of top documents \n",
    "    return from another retrieval algorithm, reranks the documents\n",
    "    using the doc2vec embedding representation and cosine similarity.\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    for query_id, top_docs in ranked_queries.items():\n",
    "        # Construct query representation\n",
    "        query_repr = np.mean(word2vec_model[raw_queries[query_id]['raw_terms']], axis=0)\n",
    "        \n",
    "        # For each document\n",
    "        for doc_id in ranked_queries[query_id]:\n",
    "            current_doc = index.document(doc_id)\n",
    "            \n",
    "            # Construct document representation\n",
    "            doc_repr = doc2vec_model.docvecs[doc_id]\n",
    "            \n",
    "            # Compute similarity score.\n",
    "            sim = cos_sim(np.reshape(query_repr, (1, -1)), np.reshape(doc_repr, (1, -1))) \n",
    "            \n",
    "            # Store similarity\n",
    "            scores[query_id].append((sim.item(), current_doc[0]))\n",
    "            \n",
    "    for query_id in scores:\n",
    "            scores[query_id] = heapq.nlargest(1000, scores[query_id], itemgetter(0))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_rerankings = doc2vec_score(ranked_queries=tfidf_queries, all_queries=raw_queries, doc2vec_model=doc2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_scores_file = 'd2vec_scores.p'\n",
    "with open(doc2vec_scores_file, 'wb') as fp:\n",
    "    pickle.dump(doc2vec_rerankings, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of TF-IDF reranking using doc2vec is {'recall_1000': 0.6089, 'P_5': 0.11, 'ndcg_cut_10': 0.1042, 'map_cut_1000': 0.0741}\n"
     ]
    }
   ],
   "source": [
    "## Use on test set\n",
    "print('Result of TF-IDF reranking using doc2vec is {0}'.format(trec_eval_test.evaluate(doc2vec_rerankings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Similarity object from sim_index.index\n",
      "INFO:gensim.utils:loaded sim_index.index\n"
     ]
    }
   ],
   "source": [
    "## Remember to load models and corpus from previous cell.\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity, Similarity\n",
    "\n",
    "similarity_index_loc = 'sim_index'\n",
    "similarity_index_file = 'sim_index.index'\n",
    "\n",
    "if not os.path.isfile(similarity_index_file):\n",
    "    # Construct similarity index\n",
    "    similarity_index = Similarity(similarity_index_loc, lsi_corpora, DIM_LSI)\n",
    "    similarity_index.save(similarity_index_file)\n",
    "else:\n",
    "    similarity_index = Similarity.load(similarity_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lsi_score(ranked_queries, all_queries, similarity_index):\n",
    "    \"\"\"\n",
    "    Using latent semantic indexing transform each query into its representation and\n",
    "    compute the cosine similarity between the query and its top 1000 docs.\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    # Set similarity index to return scores for all docs.\n",
    "    similarity_index.num_best = None\n",
    "    \n",
    "    for query_id, top_docs in ranked_queries.items():\n",
    "        query_bow = dictionary.doc2bow(query_terms)\n",
    "        query_lsi = lsi_model[tfidf_model[query_bow]]\n",
    "        \n",
    "        # Get similarity score for this query for all docuemnts\n",
    "        sims = similarity_index[query_lsi]\n",
    "        \n",
    "        # Only store the score of the documents that were iniatially ranked using tfidf.\n",
    "        for doc_id in top_docs:\n",
    "            document = index.document(doc_id)\n",
    "            \n",
    "            # Store score, account for document id - LSI id mismatch\n",
    "            scores[query_id].append((sims[doc_id-1], document[0]))\n",
    "            \n",
    "    for query_id in scores:\n",
    "            scores[query_id] = heapq.nlargest(1000, scores[query_id], itemgetter(0))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading MatrixSimilarity object from sim_index.0\n",
      "INFO:gensim.utils:loaded sim_index.0\n",
      "INFO:gensim.utils:loading MatrixSimilarity object from sim_index.1\n",
      "INFO:gensim.utils:loaded sim_index.1\n",
      "INFO:gensim.utils:loading MatrixSimilarity object from sim_index.2\n",
      "INFO:gensim.utils:loaded sim_index.2\n",
      "INFO:gensim.utils:loading MatrixSimilarity object from sim_index.3\n",
      "INFO:gensim.utils:loaded sim_index.3\n",
      "INFO:gensim.utils:loading MatrixSimilarity object from sim_index.4\n",
      "INFO:gensim.utils:loaded sim_index.4\n",
      "INFO:gensim.utils:loading MatrixSimilarity object from sim_index.5\n",
      "INFO:gensim.utils:loaded sim_index.5\n"
     ]
    }
   ],
   "source": [
    "lsi_reranking_score = lsi_score(ranked_queries=tfidf_queries, all_queries=raw_queries, similarity_index=similarity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_scores_file = 'lsi_scores.p'\n",
    "with open(lsi_scores_file, 'wb') as fp:\n",
    "    pickle.dump(lsi_reranking_score, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of TF-IDF reranking using LSI is {'recall_1000': 0.6089, 'P_5': 0.0133, 'ndcg_cut_10': 0.0159, 'map_cut_1000': 0.0485}\n"
     ]
    }
   ],
   "source": [
    "## Use on test set\n",
    "print('Result of TF-IDF reranking using LSI is {0}'.format(trec_eval_test.evaluate(lsi_reranking_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.similarities.docsim:starting similarity index under lda_sim_index\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=10000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=20000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=30000\n",
      "INFO:gensim.similarities.docsim:creating sparse index\n",
      "INFO:gensim.matutils:creating sparse matrix from corpus\n",
      "INFO:gensim.matutils:PROGRESS: at document #0/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #10000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #20000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #30000/32768\n",
      "INFO:gensim.similarities.docsim:created <32768x32 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 111082 stored elements in Compressed Sparse Row format>\n",
      "INFO:gensim.similarities.docsim:creating sparse shard #0\n",
      "INFO:gensim.similarities.docsim:saving index shard to lda_sim_index.0\n",
      "INFO:gensim.utils:saving SparseMatrixSimilarity object under lda_sim_index.0, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.0\n",
      "INFO:gensim.utils:loading SparseMatrixSimilarity object from lda_sim_index.0\n",
      "INFO:gensim.utils:loaded lda_sim_index.0\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=0\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=10000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=20000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=30000\n",
      "INFO:gensim.similarities.docsim:creating sparse index\n",
      "INFO:gensim.matutils:creating sparse matrix from corpus\n",
      "INFO:gensim.matutils:PROGRESS: at document #0/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #10000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #20000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #30000/32768\n",
      "INFO:gensim.similarities.docsim:created <32768x32 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 112878 stored elements in Compressed Sparse Row format>\n",
      "INFO:gensim.similarities.docsim:creating sparse shard #1\n",
      "INFO:gensim.similarities.docsim:saving index shard to lda_sim_index.1\n",
      "INFO:gensim.utils:saving SparseMatrixSimilarity object under lda_sim_index.1, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.1\n",
      "INFO:gensim.utils:loading SparseMatrixSimilarity object from lda_sim_index.1\n",
      "INFO:gensim.utils:loaded lda_sim_index.1\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=0\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=10000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=20000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=30000\n",
      "INFO:gensim.similarities.docsim:creating sparse index\n",
      "INFO:gensim.matutils:creating sparse matrix from corpus\n",
      "INFO:gensim.matutils:PROGRESS: at document #0/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #10000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #20000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #30000/32768\n",
      "INFO:gensim.similarities.docsim:created <32768x32 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 113681 stored elements in Compressed Sparse Row format>\n",
      "INFO:gensim.similarities.docsim:creating sparse shard #2\n",
      "INFO:gensim.similarities.docsim:saving index shard to lda_sim_index.2\n",
      "INFO:gensim.utils:saving SparseMatrixSimilarity object under lda_sim_index.2, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.2\n",
      "INFO:gensim.utils:loading SparseMatrixSimilarity object from lda_sim_index.2\n",
      "INFO:gensim.utils:loaded lda_sim_index.2\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=0\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=10000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=20000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=30000\n",
      "INFO:gensim.similarities.docsim:creating sparse index\n",
      "INFO:gensim.matutils:creating sparse matrix from corpus\n",
      "INFO:gensim.matutils:PROGRESS: at document #0/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #10000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #20000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #30000/32768\n",
      "INFO:gensim.similarities.docsim:created <32768x32 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 113631 stored elements in Compressed Sparse Row format>\n",
      "INFO:gensim.similarities.docsim:creating sparse shard #3\n",
      "INFO:gensim.similarities.docsim:saving index shard to lda_sim_index.3\n",
      "INFO:gensim.utils:saving SparseMatrixSimilarity object under lda_sim_index.3, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.3\n",
      "INFO:gensim.utils:loading SparseMatrixSimilarity object from lda_sim_index.3\n",
      "INFO:gensim.utils:loaded lda_sim_index.3\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=0\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=10000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=20000\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=30000\n",
      "INFO:gensim.similarities.docsim:creating sparse index\n",
      "INFO:gensim.matutils:creating sparse matrix from corpus\n",
      "INFO:gensim.matutils:PROGRESS: at document #0/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #10000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #20000/32768\n",
      "INFO:gensim.matutils:PROGRESS: at document #30000/32768\n",
      "INFO:gensim.similarities.docsim:created <32768x32 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 114268 stored elements in Compressed Sparse Row format>\n",
      "INFO:gensim.similarities.docsim:creating sparse shard #4\n",
      "INFO:gensim.similarities.docsim:saving index shard to lda_sim_index.4\n",
      "INFO:gensim.utils:saving SparseMatrixSimilarity object under lda_sim_index.4, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.4\n",
      "INFO:gensim.utils:loading SparseMatrixSimilarity object from lda_sim_index.4\n",
      "INFO:gensim.utils:loaded lda_sim_index.4\n",
      "INFO:gensim.similarities.docsim:PROGRESS: fresh_shard size=0\n",
      "INFO:gensim.similarities.docsim:creating sparse index\n",
      "INFO:gensim.matutils:creating sparse matrix from corpus\n",
      "INFO:gensim.matutils:PROGRESS: at document #0/757\n",
      "INFO:gensim.similarities.docsim:created <757x32 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 2664 stored elements in Compressed Sparse Row format>\n",
      "INFO:gensim.similarities.docsim:creating sparse shard #5\n",
      "INFO:gensim.similarities.docsim:saving index shard to lda_sim_index.5\n",
      "INFO:gensim.utils:saving SparseMatrixSimilarity object under lda_sim_index.5, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.5\n",
      "INFO:gensim.utils:loading SparseMatrixSimilarity object from lda_sim_index.5\n",
      "INFO:gensim.utils:loaded lda_sim_index.5\n",
      "INFO:gensim.utils:saving Similarity object under lda_sim_index.index, separately None\n",
      "INFO:gensim.utils:saved lda_sim_index.index\n"
     ]
    }
   ],
   "source": [
    "lda_similarity_index_loc = 'lda_sim_index'\n",
    "lda_similarity_index_file = 'lda_sim_index.index'\n",
    "\n",
    "if not os.path.isfile(lda_similarity_index_file):\n",
    "    # Construct similarity index\n",
    "    lda_similarity_index = Similarity(lda_similarity_index_loc, lda_corpora, DIM_LDA)\n",
    "    lda_similarity_index.save(lda_similarity_index_file)\n",
    "else:\n",
    "    lda_similarity_index = Similarity.load(lda_similarity_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_score(ranked_queries, all_queries, similarity_index):\n",
    "    \"\"\"\n",
    "    Using Latent Dirichlet Allocation transform each query into its representation and\n",
    "    compute the cosine similarity between the query and its top 1000 docs.\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    # Set similarity index to return scores for all docs.\n",
    "    similarity_index.num_best = None\n",
    "    \n",
    "    # Iterate through queries\n",
    "    for query_id, top_docs in ranked_queries.items():\n",
    "        query_bow = dictionary.doc2bow(query_terms)\n",
    "        query_lda = lda_model[query_bow]\n",
    "        \n",
    "        # Get similarity score for this query for all docuemnts\n",
    "        sims = similarity_index[query_lda]\n",
    "        \n",
    "        # Only store the score of the documents that were iniatially ranked using tfidf.\n",
    "        for doc_id in top_docs:\n",
    "            document = index.document(doc_id)\n",
    "            \n",
    "            # Store score, account for document id - LDA id mismatch\n",
    "            scores[query_id].append((sims[doc_id-1], document[0]))\n",
    "            \n",
    "    for query_id in scores:\n",
    "            scores[query_id] = heapq.nlargest(1000, scores[query_id], itemgetter(0))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_reranking_scores = lda_score(ranked_queries=tfidf_queries, all_queries=raw_queries, similarity_index=lda_similarity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_scores_file = 'lda_scores.p'\n",
    "with open(lda_scores_file, 'wb') as fp:\n",
    "    pickle.dump(lda_reranking_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of TF-IDF reranking using LSI is {'recall_1000': 0.6089, 'P_5': 0.0167, 'ndcg_cut_10': 0.0159, 'map_cut_1000': 0.0413}\n"
     ]
    }
   ],
   "source": [
    "## Use on test set\n",
    "print('Result of TF-IDF reranking using LSI is {0}'.format(trec_eval_test.evaluate(lda_reranking_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between WORD2VEC Reranking of TF-IDF and TF-IDF\n",
      "Null hypothesis of equal averages accepted for Precision@5\n",
      "Null hypothesis of equal averages accepted for Recall@1000\n",
      "Null hypothesis of equal averages accepted for NDCG@10\n",
      "Null hypothesis of equal averages accepted for MAP@1000\n",
      "\n",
      "Comparison between DOC2VEC Reranking of TF-IDF and TF-IDF\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/stats.py:3658: RuntimeWarning: invalid value encountered in absolute\n",
      "  prob = distributions.t.sf(np.abs(t), df) * 2  # use np.abs to get upper tail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null hypothesis of equal averages rejected for Precision@5\n",
      "Null hypothesis of equal averages accepted for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n",
      "\n",
      "Comparison between LSI Reranking of TF-IDF and TF-IDF\n",
      "Null hypothesis of equal averages rejected for Precision@5\n",
      "Null hypothesis of equal averages accepted for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n",
      "\n",
      "Comparison between LDA Reranking of TF-IDF and TF-IDF\n",
      "Null hypothesis of equal averages rejected for Precision@5\n",
      "Null hypothesis of equal averages accepted for Recall@1000\n",
      "Null hypothesis of equal averages rejected for NDCG@10\n",
      "Null hypothesis of equal averages rejected for MAP@1000\n"
     ]
    }
   ],
   "source": [
    "#### Statistical comparisons with TF-IDF, as they are all rerankings\n",
    "## Compare TF-IDF and best Jalinek, Dirichlet, ABS.DISC, PLM\n",
    "print('Comparison between WORD2VEC Reranking of TF-IDF and TF-IDF')\n",
    "compute_related_ttest(tf, word2vec_rerankings)\n",
    "\n",
    "print('\\nComparison between DOC2VEC Reranking of TF-IDF and TF-IDF\\n')\n",
    "compute_related_ttest(tf, doc2vec_rerankings)\n",
    "\n",
    "print('\\nComparison between LSI Reranking of TF-IDF and TF-IDF')\n",
    "compute_related_ttest(tf, lsi_reranking_score)\n",
    "\n",
    "print('\\nComparison between LDA Reranking of TF-IDF and TF-IDF')\n",
    "compute_related_ttest(tf, lda_reranking_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3: Learning to rank (LTR) [10 points] ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval. You will experiment with a pointwise learning to rank method, logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "**NOTE**: you can only perform this task if you have completely finished Task 1 and Task 2.\n",
    "\n",
    "In this experiment, you will use the retrieval methods you implemented in Task 1 and Task 2 as features for the learning to rank model. Train your LTR model using 10-fold cross validation on the test set. For every query, first create a document candidate set using the top-1000 documents using TF-IDF. Secondly, compute query-document values using the retrieval models above and use them as features. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "Your approach will definitely not be as good as the state-of-the-art since you are taking a pointwise approach, but we do not ask you to try pair- or listwise methods because they will be the main topic of the next assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [20 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Do not send us the VirtualBox, but only the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file system structure as on the VirtualBox.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
